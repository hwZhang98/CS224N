{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"run.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yAdLx8rWR_j9","colab_type":"code","outputId":"bf20499b-e21e-4799-8977-1ba85179f5be","executionInfo":{"status":"error","timestamp":1565576828322,"user_tz":-480,"elapsed":1874,"user":{"displayName":"张昊文","photoUrl":"","userId":"10153584734850914800"}},"colab":{"base_uri":"https://localhost:8080/","height":209}},"source":["#!/usr/bin/env python3\n","#python run.py train --train-src=./en_es_data/train.es --train-tgt=./en_es_data/train.en --dev-src=./en_es_data/dev.es --dev-tgt=./en_es_data/dev.en --vocab=vocab.json --cuda --max-epoch=10\n","#\n","\"\"\"\n","CS224N 2018-19: Homework 4\n","run.py: Run Script for Simple NMT Model\n","Pencheng Yin <pcyin@cs.cmu.edu>\n","Sahil Chopra <schopra8@stanford.edu>\n","\n","Usage:\n","    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n","    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n","    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n","\n","Options:\n","    -h --help                               show this screen.\n","    --cuda                                  use GPU\n","    --train-src=<file>                      train source file\n","    --train-tgt=<file>                      train target file\n","    --dev-src=<file>                        dev source file\n","    --dev-tgt=<file>                        dev target file\n","    --vocab=<file>                          vocab file\n","    --seed=<int>                            seed [default: 0]\n","    --batch-size=<int>                      batch size [default: 32]\n","    --embed-size=<int>                      embedding size [default: 256]\n","    --hidden-size=<int>                     hidden size [default: 256]\n","    --clip-grad=<float>                     gradient clipping [default: 5.0]\n","    --log-every=<int>                       log every [default: 10]\n","    --max-epoch=<int>                       max epoch [default: 30]\n","    --input-feed                            use input feeding\n","    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n","    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n","    --lr-decay=<float>                      learning rate decay [default: 0.5]\n","    --beam-size=<int>                       beam size [default: 5]\n","    --sample-size=<int>                     sample size [default: 5]\n","    --lr=<float>                            learning rate [default: 0.001]\n","    --uniform-init=<float>                  uniformly initialize all parameters [default: 0.1]\n","    --save-to=<file>                        model save path [default: model.bin]\n","    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n","    --dropout=<float>                       dropout [default: 0.3]\n","    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n","\"\"\"\n","import math\n","import sys\n","import pickle\n","import time\n","\n","import os\n","# 切换工作路径\n","path = \"/content/drive/My Drive/Colab Notebooks/a4\"\n","os.chdir(path)\n","os.listdir(path)\n","\n","from docopt import docopt\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","from nmt_model import Hypothesis, NMT\n","import numpy as np\n","from typing import List, Tuple, Dict, Set, Union\n","from tqdm import tqdm\n","from utils import read_corpus, batch_iter\n","from vocab import Vocab, VocabEntry\n","\n","import torch\n","import torch.nn.utils\n","\n","\n","def evaluate_ppl(model, dev_data, batch_size=32):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    @param model (NMT): NMT Model\n","    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n","    @param batch_size (batch size)\n","    @returns ppl (perplixty on dev sentences)\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    # no_grad() signals backend to throw away all gradients\n","    with torch.no_grad():\n","        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n","            loss = -model(src_sents, tgt_sents).sum()\n","\n","            cum_loss += loss.item()\n","            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            cum_tgt_words += tgt_word_num_to_predict\n","\n","        ppl = np.exp(cum_loss / cum_tgt_words)\n","\n","    if was_training:\n","        model.train()\n","\n","    return ppl\n","\n","\n","def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n","    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n","    @param references (List[List[str]]): a list of gold-standard reference target sentences\n","    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n","    @returns bleu_score: corpus-level BLEU score\n","    \"\"\"\n","    if references[0][0] == '<s>':\n","        references = [ref[1:-1] for ref in references]\n","    bleu_score = corpus_bleu([[ref] for ref in references],\n","                             [hyp.value for hyp in hypotheses])\n","    return bleu_score\n","\n","\n","def train(args: Dict):\n","    \"\"\" Train the NMT Model.\n","    @param args (Dict): args from cmd line\n","    \"\"\"\n","    train_data_src = read_corpus(args['--train-src'], source='src')\n","    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n","\n","    dev_data_src = read_corpus(args['--dev-src'], source='src')\n","    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n","\n","    train_data = list(zip(train_data_src, train_data_tgt))\n","    dev_data = list(zip(dev_data_src, dev_data_tgt))\n","\n","    train_batch_size = int(args['--batch-size'])\n","    clip_grad = float(args['--clip-grad'])    # 梯度裁剪\n","    valid_niter = int(args['--valid-niter'])\n","    log_every = int(args['--log-every'])\n","    model_save_path = args['--save-to']\n","\n","    vocab = Vocab.load(args['--vocab'])\n","    i = 1\n","    if i:\n","        model = NMT.load(args['MODEL_PATH'])\n","    else:\n","        model = NMT(embed_size=int(args['--embed-size']),\n","                selfhidden_size=int(args['--hidden-size']),\n","                dropout_rate=float(args['--dropout']),\n","                vocab=vocab)\n","    model.train()\n","\n","    uniform_init = float(args['--uniform-init'])\n","    if np.abs(uniform_init) > 0.:\n","        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n","        for p in model.parameters():\n","            p.data.uniform_(-uniform_init, uniform_init)\n","\n","    vocab_mask = torch.ones(len(vocab.tgt))\n","    vocab_mask[vocab.tgt['<pad>']] = 0\n","\n","    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n","    print('use device: %s' % device, file=sys.stderr)\n","\n","    model = model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n","\n","    num_trial = 0\n","    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","    cum_examples = report_examples = epoch = valid_num = 0\n","    hist_valid_scores = []\n","    train_time = begin_time = time.time()\n","    print('begin Maximum Likelihood training')\n","\n","    while True:\n","        epoch += 1\n","\n","        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","            train_iter += 1\n","\n","            optimizer.zero_grad()\n","\n","            batch_size = len(src_sents)\n","\n","            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n","            batch_loss = example_losses.sum()\n","            loss = batch_loss / batch_size\n","\n","            loss.backward()\n","\n","            # clip gradient    梯度裁剪\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","\n","            optimizer.step()\n","\n","            batch_losses_val = batch_loss.item()\n","            report_loss += batch_losses_val             # report 输出用\n","            cum_loss += batch_losses_val                # cum 验证用\n","\n","            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            report_tgt_words += tgt_words_num_to_predict\n","            cum_tgt_words += tgt_words_num_to_predict\n","            report_examples += batch_size\n","            cum_examples += batch_size\n","\n","            if train_iter % log_every == 0:\n","                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n","                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n","                                                                                         report_loss / report_examples,\n","                                                                                         math.exp(report_loss / report_tgt_words),\n","                                                                                         cum_examples,\n","                                                                                         report_tgt_words / (time.time() - train_time),\n","                                                                                         time.time() - begin_time), file=sys.stderr)\n","\n","                train_time = time.time()\n","                report_loss = report_tgt_words = report_examples = 0.\n","\n","            # perform validation\n","            if train_iter % valid_niter == 0:\n","                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n","                                                                                         cum_loss / cum_examples,\n","                                                                                         np.exp(cum_loss / cum_tgt_words),\n","                                                                                         cum_examples), file=sys.stderr)\n","\n","                cum_loss = cum_examples = cum_tgt_words = 0.\n","                valid_num += 1\n","\n","                print('begin validation ...', file=sys.stderr)\n","\n","                # compute dev. ppl and bleu\n","                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n","                valid_metric = -dev_ppl\n","\n","                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n","\n","                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n","                hist_valid_scores.append(valid_metric)\n","\n","                if is_better:\n","                    patience = 0\n","                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n","                    model.save(model_save_path)\n","\n","                    # also save the optimizers' state\n","                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n","                elif patience < int(args['--patience']):\n","                    patience += 1\n","                    print('hit patience %d' % patience, file=sys.stderr)\n","\n","                    if patience == int(args['--patience']):\n","                        num_trial += 1\n","                        print('hit #%d trial' % num_trial, file=sys.stderr)\n","                        if num_trial == int(args['--max-num-trial']):\n","                            print('early stop!', file=sys.stderr)\n","                            exit(0)\n","\n","                        # decay lr, and restore from previously best checkpoint\n","                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n","                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n","\n","                        # load model\n","                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n","                        model.load_state_dict(params['state_dict'])\n","                        model = model.to(device)\n","\n","                        print('restore parameters of the optimizers', file=sys.stderr)\n","                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n","\n","                        # set new lr\n","                        for param_group in optimizer.param_groups:\n","                            param_group['lr'] = lr\n","\n","                        # reset patience\n","                        patience = 0\n","\n","                if epoch == int(args['--max-epoch']):\n","                    print('reached maximum number of epochs!', file=sys.stderr)\n","                    exit(0)\n","\n","\n","def decode(args: Dict[str, str]):\n","    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n","    If the target gold-standard sentences are given, the function also computes\n","    corpus-level BLEU score.\n","    @param args (Dict): args from cmd line\n","    \"\"\"\n","\n","    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n","    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n","    if args['TEST_TARGET_FILE']:\n","        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']), file=sys.stderr)\n","        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n","\n","    print(\"load model from {}\".format(args['MODEL_PATH']), file=sys.stderr)\n","    model = NMT.load(args['MODEL_PATH'])\n","\n","    if args['--cuda']:\n","        model = model.to(torch.device(\"cuda:0\"))\n","\n","    hypotheses = beam_search(model, test_data_src,\n","                             beam_size=int(args['--beam-size']),\n","                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n","\n","    if args['TEST_TARGET_FILE']:\n","        top_hypotheses = [hyps[0] for hyps in hypotheses]\n","        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n","        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n","\n","    with open(args['OUTPUT_FILE'], 'w') as f:\n","        for src_sent, hyps in zip(test_data_src, hypotheses):\n","            top_hyp = hyps[0]\n","            hyp_sent = ' '.join(top_hyp.value)\n","            f.write(hyp_sent + '\\n')\n","\n","\n","def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n","    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n","    @param model (NMT): NMT Model\n","    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n","    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n","    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n","    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    hypotheses = []\n","    with torch.no_grad():\n","        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n","            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n","\n","            hypotheses.append(example_hyps)\n","\n","    if was_training: model.train(was_training)\n","\n","    return hypotheses\n","\n","\n","def main():\n","    \"\"\" Main func.\n","    \"\"\"\n","    args = docopt(__doc__)\n","\n","    # Check pytorch version\n","    assert(torch.__version__ == \"1.1.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n","\n","    # seed the random number generators\n","    seed = int(args['--seed'])\n","    torch.manual_seed(seed)\n","    if args['--cuda']:\n","        torch.cuda.manual_seed(seed)\n","    np.random.seed(seed * 13 // 7)\n","\n","    if args['train']:\n","        train(args)\n","    elif args['decode']:\n","        decode(args)\n","    else:\n","        raise RuntimeError('invalid run mode')\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n"],"execution_count":26,"outputs":[{"output_type":"error","ename":"DocoptExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mDocoptExit\u001b[0m\u001b[0;31m:\u001b[0m Usage:\n    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1_f8UCjucBlk","colab_type":"code","outputId":"baae9606-3232-4591-a071-01bbd99cc5ec","executionInfo":{"status":"ok","timestamp":1565576824023,"user_tz":-480,"elapsed":1413,"user":{"displayName":"张昊文","photoUrl":"","userId":"10153584734850914800"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q7vwtiFHcDqJ","colab_type":"code","outputId":"c057f15b-daa0-452e-c373-3bbb3e3d8363","executionInfo":{"status":"ok","timestamp":1565576370881,"user_tz":-480,"elapsed":7892,"user":{"displayName":"张昊文","photoUrl":"","userId":"10153584734850914800"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run.py train --train-src=./en_es_data/train.es --train-tgt=./en_es_data/train.en --dev-src=./en_es_data/dev.es --dev-tgt=./en_es_data/dev.en --vocab=vocab.json --cuda --max-epoch=10"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","uniformly initialize parameters [-0.100000, +0.100000]\n","use device: cuda:0\n","begin Maximum Likelihood training\n","epoch 1, iter 10, avg. loss 167.56, avg. ppl 24013.87 cum. examples 320, speed 5353.04 words/sec, time elapsed 0.99 sec\n","epoch 1, iter 20, avg. loss 141.97, avg. ppl 3113.04 cum. examples 640, speed 6348.82 words/sec, time elapsed 1.88 sec\n","epoch 1, iter 30, avg. loss 138.04, avg. ppl 1790.88 cum. examples 960, speed 6306.89 words/sec, time elapsed 2.82 sec\n","epoch 1, iter 40, avg. loss 131.54, avg. ppl 1746.97 cum. examples 1280, speed 5938.68 words/sec, time elapsed 3.77 sec\n","epoch 1, iter 50, avg. loss 132.29, avg. ppl 1545.48 cum. examples 1600, speed 6441.74 words/sec, time elapsed 4.66 sec\n","epoch 1, iter 60, avg. loss 126.90, avg. ppl 1346.65 cum. examples 1920, speed 6323.97 words/sec, time elapsed 5.55 sec\n","epoch 1, iter 70, avg. loss 117.25, avg. ppl 1264.41 cum. examples 2240, speed 6081.89 words/sec, time elapsed 6.42 sec\n","epoch 1, iter 80, avg. loss 132.83, avg. ppl 1205.74 cum. examples 2560, speed 6426.73 words/sec, time elapsed 7.35 sec\n","epoch 1, iter 90, avg. loss 122.80, avg. ppl 1150.94 cum. examples 2880, speed 6121.23 words/sec, time elapsed 8.26 sec\n","epoch 1, iter 100, avg. loss 118.02, avg. ppl 1097.04 cum. examples 3200, speed 5863.77 words/sec, time elapsed 9.18 sec\n","epoch 1, iter 110, avg. loss 117.96, avg. ppl 1048.79 cum. examples 3520, speed 6314.85 words/sec, time elapsed 10.04 sec\n","epoch 1, iter 120, avg. loss 115.82, avg. ppl 1011.01 cum. examples 3840, speed 6008.78 words/sec, time elapsed 10.93 sec\n","epoch 1, iter 130, avg. loss 118.71, avg. ppl 910.51 cum. examples 4160, speed 6011.66 words/sec, time elapsed 11.86 sec\n","epoch 1, iter 140, avg. loss 119.27, avg. ppl 935.34 cum. examples 4480, speed 6108.33 words/sec, time elapsed 12.77 sec\n","epoch 1, iter 150, avg. loss 118.76, avg. ppl 845.25 cum. examples 4800, speed 6205.50 words/sec, time elapsed 13.68 sec\n","epoch 1, iter 160, avg. loss 113.74, avg. ppl 852.17 cum. examples 5120, speed 5683.33 words/sec, time elapsed 14.63 sec\n","epoch 1, iter 170, avg. loss 121.95, avg. ppl 855.55 cum. examples 5440, speed 6609.69 words/sec, time elapsed 15.50 sec\n","epoch 1, iter 180, avg. loss 120.57, avg. ppl 823.59 cum. examples 5760, speed 6249.26 words/sec, time elapsed 16.42 sec\n","epoch 1, iter 190, avg. loss 113.54, avg. ppl 789.44 cum. examples 6080, speed 6079.58 words/sec, time elapsed 17.32 sec\n","epoch 1, iter 200, avg. loss 113.64, avg. ppl 706.66 cum. examples 6400, speed 6270.64 words/sec, time elapsed 18.20 sec\n","epoch 1, iter 210, avg. loss 120.42, avg. ppl 777.73 cum. examples 6720, speed 6405.52 words/sec, time elapsed 19.11 sec\n","epoch 1, iter 220, avg. loss 117.12, avg. ppl 673.32 cum. examples 7040, speed 6201.84 words/sec, time elapsed 20.04 sec\n","epoch 1, iter 230, avg. loss 116.25, avg. ppl 771.70 cum. examples 7360, speed 5936.78 words/sec, time elapsed 20.98 sec\n","epoch 1, iter 240, avg. loss 117.81, avg. ppl 659.89 cum. examples 7680, speed 6633.44 words/sec, time elapsed 21.85 sec\n","epoch 1, iter 250, avg. loss 109.75, avg. ppl 591.06 cum. examples 8000, speed 5882.68 words/sec, time elapsed 22.79 sec\n","epoch 1, iter 260, avg. loss 116.28, avg. ppl 624.33 cum. examples 8320, speed 6158.46 words/sec, time elapsed 23.73 sec\n","epoch 1, iter 270, avg. loss 114.09, avg. ppl 661.12 cum. examples 8640, speed 6278.18 words/sec, time elapsed 24.62 sec\n","epoch 1, iter 280, avg. loss 116.85, avg. ppl 643.44 cum. examples 8960, speed 5782.30 words/sec, time elapsed 25.62 sec\n","epoch 1, iter 290, avg. loss 109.43, avg. ppl 578.75 cum. examples 9280, speed 6094.41 words/sec, time elapsed 26.53 sec\n","epoch 1, iter 300, avg. loss 108.19, avg. ppl 600.63 cum. examples 9600, speed 5990.52 words/sec, time elapsed 27.43 sec\n","epoch 1, iter 310, avg. loss 116.55, avg. ppl 561.73 cum. examples 9920, speed 5996.13 words/sec, time elapsed 28.41 sec\n","epoch 1, iter 320, avg. loss 108.60, avg. ppl 543.41 cum. examples 10240, speed 6233.78 words/sec, time elapsed 29.30 sec\n","epoch 1, iter 330, avg. loss 117.08, avg. ppl 562.28 cum. examples 10560, speed 6661.93 words/sec, time elapsed 30.19 sec\n","epoch 1, iter 340, avg. loss 112.78, avg. ppl 528.92 cum. examples 10880, speed 6122.22 words/sec, time elapsed 31.13 sec\n","epoch 1, iter 350, avg. loss 112.17, avg. ppl 545.58 cum. examples 11200, speed 6362.68 words/sec, time elapsed 32.02 sec\n","epoch 1, iter 360, avg. loss 114.75, avg. ppl 539.72 cum. examples 11520, speed 5975.39 words/sec, time elapsed 33.00 sec\n","epoch 1, iter 370, avg. loss 106.34, avg. ppl 457.53 cum. examples 11840, speed 6098.15 words/sec, time elapsed 33.91 sec\n","epoch 1, iter 380, avg. loss 101.66, avg. ppl 517.33 cum. examples 12160, speed 5783.54 words/sec, time elapsed 34.81 sec\n","epoch 1, iter 390, avg. loss 115.57, avg. ppl 488.68 cum. examples 12480, speed 6696.30 words/sec, time elapsed 35.70 sec\n","epoch 1, iter 400, avg. loss 102.00, avg. ppl 448.84 cum. examples 12800, speed 5743.71 words/sec, time elapsed 36.63 sec\n","epoch 1, iter 410, avg. loss 108.21, avg. ppl 458.78 cum. examples 13120, speed 5847.81 words/sec, time elapsed 37.60 sec\n","epoch 1, iter 420, avg. loss 106.93, avg. ppl 441.70 cum. examples 13440, speed 6163.49 words/sec, time elapsed 38.51 sec\n","epoch 1, iter 430, avg. loss 104.94, avg. ppl 460.56 cum. examples 13760, speed 5778.15 words/sec, time elapsed 39.46 sec\n","epoch 1, iter 440, avg. loss 107.14, avg. ppl 437.09 cum. examples 14080, speed 6505.67 words/sec, time elapsed 40.32 sec\n","epoch 1, iter 450, avg. loss 107.96, avg. ppl 477.75 cum. examples 14400, speed 6333.91 words/sec, time elapsed 41.21 sec\n","epoch 1, iter 460, avg. loss 102.86, avg. ppl 439.23 cum. examples 14720, speed 6049.77 words/sec, time elapsed 42.10 sec\n","epoch 1, iter 470, avg. loss 112.92, avg. ppl 459.37 cum. examples 15040, speed 6305.19 words/sec, time elapsed 43.04 sec\n","epoch 1, iter 480, avg. loss 104.69, avg. ppl 394.29 cum. examples 15360, speed 5628.24 words/sec, time elapsed 44.03 sec\n"],"name":"stdout"}]}]}