{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#python run.py train --train-src=./en_es_data/train.es --train-tgt=./en_es_data/train.en --dev-src=./en_es_data/dev.es --dev-tgt=./en_es_data/dev.en --vocab=vocab.json\n",
    "#\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "run.py: Run Script for Simple NMT Model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\n",
    "Usage:\n",
    "    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n",
    "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n",
    "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n",
    "\n",
    "Options:\n",
    "    -h --help                               show this screen.\n",
    "    --cuda                                  use GPU\n",
    "    --train-src=<file>                      train source file\n",
    "    --train-tgt=<file>                      train target file\n",
    "    --dev-src=<file>                        dev source file\n",
    "    --dev-tgt=<file>                        dev target file\n",
    "    --vocab=<file>                          vocab file\n",
    "    --seed=<int>                            seed [default: 0]\n",
    "    --batch-size=<int>                      batch size [default: 32]\n",
    "    --embed-size=<int>                      embedding size [default: 256]\n",
    "    --hidden-size=<int>                     hidden size [default: 256]\n",
    "    --clip-grad=<float>                     gradient clipping [default: 5.0]\n",
    "    --log-every=<int>                       log every [default: 10]\n",
    "    --max-epoch=<int>                       max epoch [default: 30]\n",
    "    --input-feed                            use input feeding\n",
    "    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n",
    "    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n",
    "    --lr-decay=<float>                      learning rate decay [default: 0.5]\n",
    "    --beam-size=<int>                       beam size [default: 5]\n",
    "    --sample-size=<int>                     sample size [default: 5]\n",
    "    --lr=<float>                            learning rate [default: 0.001]\n",
    "    --uniform-init=<float>                  uniformly initialize all parameters [default: 0.1]\n",
    "    --save-to=<file>                        model save path [default: model.bin]\n",
    "    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n",
    "    --dropout=<float>                       dropout [default: 0.3]\n",
    "    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n",
    "\"\"\"\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "from docopt import docopt\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "from nmt_model import Hypothesis, NMT\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "from utils import read_corpus, batch_iter\n",
    "from vocab import Vocab, VocabEntry\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "\n",
    "\n",
    "def evaluate_ppl(model, dev_data, batch_size=32):\n",
    "    \"\"\" Evaluate perplexity on dev sentences\n",
    "    @param model (NMT): NMT Model\n",
    "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (batch size)\n",
    "    @returns ppl (perplixty on dev sentences)\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    cum_loss = 0.\n",
    "    cum_tgt_words = 0.\n",
    "\n",
    "    # no_grad() signals backend to throw away all gradients\n",
    "    with torch.no_grad():\n",
    "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
    "            loss = -model(src_sents, tgt_sents).sum()\n",
    "\n",
    "            cum_loss += loss.item()\n",
    "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
    "            cum_tgt_words += tgt_word_num_to_predict\n",
    "\n",
    "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
    "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
    "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
    "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
    "    @returns bleu_score: corpus-level BLEU score\n",
    "    \"\"\"\n",
    "    if references[0][0] == '<s>':\n",
    "        references = [ref[1:-1] for ref in references]\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
    "                             [hyp.value for hyp in hypotheses])\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def train(args: Dict):\n",
    "    \"\"\" Train the NMT Model.\n",
    "    @param args (Dict): args from cmd line\n",
    "    \"\"\"\n",
    "    train_data_src = read_corpus(args['--train-src'], source='src')\n",
    "    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n",
    "\n",
    "    dev_data_src = read_corpus(args['--dev-src'], source='src')\n",
    "    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n",
    "\n",
    "    train_data = list(zip(train_data_src, train_data_tgt))\n",
    "    dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
    "\n",
    "    train_batch_size = int(args['--batch-size'])\n",
    "    clip_grad = float(args['--clip-grad'])    # 梯度裁剪\n",
    "    valid_niter = int(args['--valid-niter'])\n",
    "    log_every = int(args['--log-every'])\n",
    "    model_save_path = args['--save-to']\n",
    "\n",
    "    vocab = Vocab.load(args['--vocab'])\n",
    "\n",
    "    model = NMT(embed_size=int(args['--embed-size']),\n",
    "                selfhidden_size=int(args['--hidden-size']),\n",
    "                dropout_rate=float(args['--dropout']),\n",
    "                vocab=vocab)\n",
    "    model.train()\n",
    "\n",
    "    uniform_init = float(args['--uniform-init'])\n",
    "    if np.abs(uniform_init) > 0.:\n",
    "        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n",
    "        for p in model.parameters():\n",
    "            p.data.uniform_(-uniform_init, uniform_init)\n",
    "\n",
    "    vocab_mask = torch.ones(len(vocab.tgt))\n",
    "    vocab_mask[vocab.tgt['<pad>']] = 0\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n",
    "    print('use device: %s' % device, file=sys.stderr)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n",
    "\n",
    "    num_trial = 0\n",
    "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
    "    cum_examples = report_examples = epoch = valid_num = 0\n",
    "    hist_valid_scores = []\n",
    "    train_time = begin_time = time.time()\n",
    "    print('begin Maximum Likelihood training')\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "\n",
    "        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n",
    "            train_iter += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size = len(src_sents)\n",
    "\n",
    "            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n",
    "            batch_loss = example_losses.sum()\n",
    "            loss = batch_loss / batch_size\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradient    梯度裁剪\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses_val = batch_loss.item()\n",
    "            report_loss += batch_losses_val             # report 输出用\n",
    "            cum_loss += batch_losses_val                # cum 验证用\n",
    "\n",
    "            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
    "            report_tgt_words += tgt_words_num_to_predict\n",
    "            cum_tgt_words += tgt_words_num_to_predict\n",
    "            report_examples += batch_size\n",
    "            cum_examples += batch_size\n",
    "\n",
    "            if train_iter % log_every == 0:\n",
    "                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
    "                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
    "                                                                                         report_loss / report_examples,\n",
    "                                                                                         math.exp(report_loss / report_tgt_words),\n",
    "                                                                                         cum_examples,\n",
    "                                                                                         report_tgt_words / (time.time() - train_time),\n",
    "                                                                                         time.time() - begin_time), file=sys.stderr)\n",
    "\n",
    "                train_time = time.time()\n",
    "                report_loss = report_tgt_words = report_examples = 0.\n",
    "\n",
    "            # perform validation\n",
    "            if train_iter % valid_niter == 0:\n",
    "                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
    "                                                                                         cum_loss / cum_examples,\n",
    "                                                                                         np.exp(cum_loss / cum_tgt_words),\n",
    "                                                                                         cum_examples), file=sys.stderr)\n",
    "\n",
    "                cum_loss = cum_examples = cum_tgt_words = 0.\n",
    "                valid_num += 1\n",
    "\n",
    "                print('begin validation ...', file=sys.stderr)\n",
    "\n",
    "                # compute dev. ppl and bleu\n",
    "                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
    "                valid_metric = -dev_ppl\n",
    "\n",
    "                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n",
    "\n",
    "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
    "                hist_valid_scores.append(valid_metric)\n",
    "\n",
    "                if is_better:\n",
    "                    patience = 0\n",
    "                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
    "                    model.save(model_save_path)\n",
    "\n",
    "                    # also save the optimizers' state\n",
    "                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
    "                elif patience < int(args['--patience']):\n",
    "                    patience += 1\n",
    "                    print('hit patience %d' % patience, file=sys.stderr)\n",
    "\n",
    "                    if patience == int(args['--patience']):\n",
    "                        num_trial += 1\n",
    "                        print('hit #%d trial' % num_trial, file=sys.stderr)\n",
    "                        if num_trial == int(args['--max-num-trial']):\n",
    "                            print('early stop!', file=sys.stderr)\n",
    "                            exit(0)\n",
    "\n",
    "                        # decay lr, and restore from previously best checkpoint\n",
    "                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n",
    "                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n",
    "\n",
    "                        # load model\n",
    "                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
    "                        model.load_state_dict(params['state_dict'])\n",
    "                        model = model.to(device)\n",
    "\n",
    "                        print('restore parameters of the optimizers', file=sys.stderr)\n",
    "                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
    "\n",
    "                        # set new lr\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "\n",
    "                        # reset patience\n",
    "                        patience = 0\n",
    "\n",
    "                if epoch == int(args['--max-epoch']):\n",
    "                    print('reached maximum number of epochs!', file=sys.stderr)\n",
    "                    exit(0)\n",
    "\n",
    "\n",
    "def decode(args: Dict[str, str]):\n",
    "    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n",
    "    If the target gold-standard sentences are given, the function also computes\n",
    "    corpus-level BLEU score.\n",
    "    @param args (Dict): args from cmd line\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n",
    "    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n",
    "    if args['TEST_TARGET_FILE']:\n",
    "        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']), file=sys.stderr)\n",
    "        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n",
    "\n",
    "    print(\"load model from {}\".format(args['MODEL_PATH']), file=sys.stderr)\n",
    "    model = NMT.load(args['MODEL_PATH'])\n",
    "\n",
    "    if args['--cuda']:\n",
    "        model = model.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "    hypotheses = beam_search(model, test_data_src,\n",
    "                             beam_size=int(args['--beam-size']),\n",
    "                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n",
    "\n",
    "    if args['TEST_TARGET_FILE']:\n",
    "        top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
    "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
    "        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n",
    "\n",
    "    with open(args['OUTPUT_FILE'], 'w') as f:\n",
    "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
    "            top_hyp = hyps[0]\n",
    "            hyp_sent = ' '.join(top_hyp.value)\n",
    "            f.write(hyp_sent + '\\n')\n",
    "\n",
    "\n",
    "def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
    "    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n",
    "    @param model (NMT): NMT Model\n",
    "    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n",
    "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
    "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
    "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    hypotheses = []\n",
    "    with torch.no_grad():\n",
    "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
    "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
    "\n",
    "            hypotheses.append(example_hyps)\n",
    "\n",
    "    if was_training: model.train(was_training)\n",
    "\n",
    "    return hypotheses\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main func.\n",
    "    \"\"\"\n",
    "    args = docopt(__doc__)\n",
    "\n",
    "    # Check pytorch version\n",
    "    assert(torch.__version__ == \"1.1.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n",
    "\n",
    "    # seed the random number generators\n",
    "    seed = int(args['--seed'])\n",
    "    torch.manual_seed(seed)\n",
    "    if args['--cuda']:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed * 13 // 7)\n",
    "\n",
    "    if args['train']:\n",
    "        train(args)\n",
    "    elif args['decode']:\n",
    "        decode(args)\n",
    "    else:\n",
    "        raise RuntimeError('invalid run mode')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
